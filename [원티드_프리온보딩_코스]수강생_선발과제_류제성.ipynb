{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[원티드 프리온보딩 코스]수강생_선발과제_류제성.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 과제 안내\n",
        "\n",
        "- 아래 설명에 따라 코드의 빈칸을 채워 Tokenizer(문제 1)와 TfidfVectorizer(문제 2) 클래스를 완성하세요.\n",
        "    - 문제 1, 문제 2 모두 수행해야 합니다.\n",
        "    - 주어진 조건을 모두 만족해야 합니다.\n",
        "- 작업한 파일을 하나의 GitHub Repository에 담아서 제출하세요.\n",
        "    - 파일 형식: `.ipynb`\n",
        "        - `.ipynb` 파일 하나에 문제 1과 문제 2 작업 결과를 모두 담아 주시기 바랍니다.\n",
        "    - 링크 제출 전 해당 GitHub Repository가 public으로 설정되어 있는지 확인 바랍니다.\n",
        "        - private으로 설정 시 제출 확인이 불가합니다.\n",
        "\n",
        "## 사용 권장 기술\n",
        "\n",
        "- 사용 언어: Python (필수)\n",
        "- 외부 라이브러리 사용은 자유이나, output의 type은 문제에 명시된 조건을 따라야 합니다."
      ],
      "metadata": {
        "id": "xqGQdPm3ssGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    '''\n",
        "    문제 1-1.\n",
        "    '''\n",
        "    for sen in sequences:\n",
        "      sen = sen.lower() # to lower letter\n",
        "      sen = re.sub(\"[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]\",\"\",sen) # delete special characters\n",
        "      sen = sen.split(' ') #split by white space\n",
        "      result.append(sen)\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    '''\n",
        "    문제 1-2.\n",
        "    '''\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    for sen in tokens:\n",
        "      for word in sen:\n",
        "        if word not in self.word_dict:\n",
        "            self.word_dict[word] = max(self.word_dict.values()) +1 # 최종 단어장 value보다 1 큰 값으로 매핑\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    # print(tokens)\n",
        "    if self.fit_checker:\n",
        "      '''\n",
        "      문제 1-3.\n",
        "      '''\n",
        "      for sen in tokens:\n",
        "        mapped_seq = list(map(lambda x : self.word_dict[x] if x in self.word_dict else self.word_dict['oov'],sen))\n",
        "        # print(mapped_seq)\n",
        "        result.append(mapped_seq)\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "_1IgyzGUqsvQ"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 1) Tokenizer 생성하기**\n",
        "\n",
        "**1-1. `preprocessing()`**\n",
        "\n",
        "텍스트 전처리를 하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
        "- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
        "- 조건 2: 토큰화는 white space 단위로 수행합니다."
      ],
      "metadata": {
        "id": "6GTjj8ykp4qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq = ['I go to school.', 'I LIKE pizza!', 'Love is, never having to say you are sorry...']\n",
        "\n",
        "tok = Tokenizer()\n",
        "tok.preprocessing(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoEgOkksuzpq",
        "outputId": "fc51fe8a-692d-454d-d59b-7b5de70093f0"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i', 'go', 'to', 'school'],\n",
              " ['i', 'like', 'pizza'],\n",
              " ['love', 'is', 'never', 'having', 'to', 'say', 'you', 'are', 'sorry']]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-2. `fit()`**\n",
        "\n",
        "어휘 사전을 구축하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
        "- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
        "    - 주어진 코드에 있는 `self.word_dict`를 활용합니다."
      ],
      "metadata": {
        "id": "rD7-VK2hqAao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seq = ['I go to school.', 'I LIKE pizza!', 'Love is, never having to say you are sorry...']\n",
        "\n",
        "tok.fit(seq)\n",
        "print(tok.word_dict)\n",
        "# tok.fit_checker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAjvNckBqBGY",
        "outputId": "4231bf74-fe77-402a-a1e9-55ba6cec7933"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6, 'love': 7, 'is': 8, 'never': 9, 'having': 10, 'say': 11, 'you': 12, 'are': 13, 'sorry': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-3. `transform()`**\n",
        "\n",
        "어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n",
        "- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."
      ],
      "metadata": {
        "id": "6HyhvLXVqBpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit 했던 sequence와 다른 문장 data로 tokenize \n",
        "\n",
        "sequences = ['I love you so much', 'PIZZA is my favorite!!', 'I am sorry. It is my mistake.']\n",
        "\n",
        "tok.transform(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z51xQ5gEqCMH",
        "outputId": "0191b91a-dc27-4aae-b290-3ccc2a06f7cd"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 7, 12, 0, 0], [6, 8, 0, 0], [1, 0, 14, 0, 8, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    '''\n",
        "    문제 2-1.\n",
        "    '''\n",
        "    # calculate df(d,t)\n",
        "    num_token = set(sum(tokenized,[]))\n",
        "    dic={}\n",
        "    for num in num_token:\n",
        "      cnt=0\n",
        "      for sen in tokenized:\n",
        "        if num in set(sen):\n",
        "          cnt+=1\n",
        "      dic[num]=cnt\n",
        "\n",
        "    # calculate idf \n",
        "    N=len(sequences)\n",
        "    result=[]\n",
        "    for sen in tokenized:\n",
        "      result.append([np.log(N/(dic[token]+1)) for token in sen])\n",
        "    self.idf = result\n",
        "       \n",
        "    \n",
        "    \n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      '''\n",
        "      ## term frequency\n",
        "      self.tf = [list(map(lambda x : sen.count(x),sen)) for sen in tokenized]\n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "      # now multipy tf * idf\n",
        "\n",
        "      self.tfidf_matrix=[]\n",
        "      for tf,idf in zip(self.tf, self.idf):\n",
        "        self.tfidf_matrix.append([a*b for a, b in zip(tf,idf)])\n",
        "      \n",
        "      # print(self.tf)\n",
        "      # print(self.idf)\n",
        "          \n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "FF8b9vKQqxhY"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 2) TfidfVectorizer 생성하기**\n",
        "\n",
        "**2-1. `fit()`**\n",
        "\n",
        "입력 문장들을 이용해 IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: IDF 행렬은 list 형태입니다.\n",
        "    - ex) [토큰1에 대한 IDF 값, 토큰2에 대한 IDF 값, .... ]\n",
        "- 조건 2: IDF 값은 아래 식을 이용해 구합니다.\n",
        "$$idf(d,t)=log_e(\\frac{n}{1+df(d,t)})$$\n",
        "  - $df(d,t)$ : 단어 t가 포함된 문장 d의 개수\n",
        "  - $n$ : 입력된 전체 문장 개수\n",
        "\n",
        "- 조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다."
      ],
      "metadata": {
        "id": "X50kWQ6mqLWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq = ['I love to go to school.', 'I love pizza, pizza!', 'Love is, never having to say you are sorry...']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "vectorizer =TfidfVectorizer(tokenizer) # tokenizer 통해 생성\n",
        "vectorizer.fit(seq)\n",
        "\n",
        "vectorizer.idf #instance의 attribute로 할당"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gBeRCITqQsO",
        "outputId": "9849053b-faf0-4cf1-dad2-cdd6d7f4f1fb"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, -0.2876820724517809, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644],\n",
              " [0.0, -0.2876820724517809, 0.4054651081081644, 0.4054651081081644],\n",
              " [-0.2876820724517809,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.0,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644]]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2-2. `transform()`**\n",
        "\n",
        "입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output : nested list 형태입니다.\n",
        "    \n",
        "    ex) [[tf-idf(1, 1), tf-idf(1, 2), tf-idf(1, 3)], [tf-idf(2, 1), tf-idf(2, 2), tf-idf(2, 3)]]\n",
        "    \n",
        "    ![image.png](data:image/png;base64,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)\n",
        "- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
        "    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n",
        "- 조건2 : 문제 2-1( `fit()`)에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
        "    \n",
        "$$tf-idf(d,t) = tf(d,t) \\times idf(d,t)$$"
      ],
      "metadata": {
        "id": "EWGUMVr-qRen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.transform(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjV3GlnoqTQ_",
        "outputId": "9f3d9349-8f19-4d0f-9a03-82df79017d5d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, -0.2876820724517809, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644],\n",
              " [0.0, -0.2876820724517809, 0.8109302162163288, 0.8109302162163288],\n",
              " [-0.2876820724517809,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.0,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644]]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    }
  ]
}